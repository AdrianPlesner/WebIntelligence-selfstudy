{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self study 2\n",
    "\n",
    "\n",
    "In this self-study we build an index that supports Boolean search over the web pages that you crawl with the crawler from the 1st self study. You can continue to just extract the titles of the web-pages you crawl, or you can be more adventurous and look at the whole text that you get from the .get_text() method of a BeautifulSoup parser. In either case, the collection of texts from the crawled web-pages is you corpus. You should then:\n",
    "\n",
    "- construct the vocabulary of terms for your corpus\n",
    "- build an 'inverted' index for your vocabulary\n",
    "- implement Boolean search for your index ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some things already used in self study 1:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful resource is the nltk natural language processing package:\n",
    "https://www.nltk.org/\n",
    "which provides methods for tokenization, stemming, and much more (the 'punkt' package is needed for tokenization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the title string of the AAU homepage as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=requests.get('https://www.aau.dk/')\n",
    "r_parse = BeautifulSoup(r.text, 'html.parser')\n",
    "string=r_parse.find('title').string\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=nltk.word_tokenize(string)\n",
    "for t in tokens:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can stem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=nltk.PorterStemmer()\n",
    "for t in tokens:\n",
    "    print(ps.stem(t))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Danish language the Porter stemmer will not be terribly useful! There is also a Danish option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "dstemmer=SnowballStemmer(\"danish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tokens:\n",
    "    print(dstemmer.stem(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is most useful for you depends on which websites you crawl. It is not essential for the exercise that the stemming always is the best possible ...!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from queue import SimpleQueue as Queue\n",
    "import random\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "\n",
    "# Keep lists of visited pages and a result of urls and titles\n",
    "visited = []\n",
    "results = []\n",
    "\n",
    "# Initialize frontier of 10 front queues that are assigned randomly\n",
    "frontier = []\n",
    "for i in range(4):\n",
    "    frontier.append(Queue())\n",
    "\n",
    "def enqueue(qlist, obj):\n",
    "    q = random.choice(qlist)\n",
    "    q.put(obj)\n",
    "\n",
    "\n",
    "\n",
    "# Keep back queues as dictionary\n",
    "backQ = {}\n",
    "\n",
    "# Extract the next url crawl from back queues\n",
    "def get_crawl(qd):\n",
    "    result = None\n",
    "    keys = list(qd)\n",
    "    i = 0\n",
    "    sec2 = timedelta(0,2)\n",
    "    # Search through each back queue\n",
    "    keys.sort(key=lambda x: qd[x]['time'])\n",
    "    while result is None and i < len(keys):\n",
    "        key = keys[i]\n",
    "        if not qd[key]['queue'].empty() and datetime.now() > qd[key]['time']:\n",
    "            # This queue is not empty and the timestamp permits\n",
    "            result = qd[key]['queue'].get()\n",
    "            # Update with new timestamp\n",
    "            qd[key]['time'] = datetime.now() + sec2\n",
    "\n",
    "        else:\n",
    "            i += 1\n",
    "    return result\n",
    "\n",
    "# Approximate host domain\n",
    "def extract_domain(url):\n",
    "    s = url.split(\"/\")\n",
    "    return s[2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define start seed\n",
    "seed = [\"https://pinchofyum.com/\", \"https://loveandlemons.com/\",\"https://imdb.com/\", \"https://gamegrumps.com/\", \"https://www.aau.dk/\"]\n",
    "\n",
    "# Put seeds in frontier\n",
    "for s in seed:\n",
    "    enqueue(frontier, s)\n",
    "\n",
    "index = 0\n",
    "\n",
    "while len(results) < 1000:\n",
    "    next_url = get_crawl(backQ)\n",
    "    if not backQ or get_crawl(backQ) is None:\n",
    "        # If all back queues are empty, refill by emptying front queues\n",
    "        for f in frontier:\n",
    "            while not f.empty():\n",
    "                url = f.get()\n",
    "                domain = extract_domain(url)\n",
    "                if domain not in backQ.keys():\n",
    "                    # Add new back queue if one for this domain does not exist\n",
    "                    backQ[domain] = {'time': datetime.now(), 'queue': Queue()}\n",
    "                backQ[domain]['queue'].put(url)\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            # Logic for crawling a page\n",
    "            if next_url in visited or next_url is None:\n",
    "                continue\n",
    "            print('crawling at ' + next_url)\n",
    "            # Initialize robotfile parser\n",
    "            rp=RobotFileParser()\n",
    "            rp.set_url(next_url)\n",
    "            rp.read()\n",
    "            # Check if robots.txt allows\n",
    "            if rp.can_fetch(\"*\", next_url):\n",
    "                r=requests.get(next_url)\n",
    "                visited.append(next_url)\n",
    "                #extract title\n",
    "                r_parse = BeautifulSoup(r.text, 'html.parser')\n",
    "                title = r_parse.find('title')\n",
    "                if title is not None:\n",
    "                    title = title.string\n",
    "                    # Get text from page and tokenize\n",
    "                    text = r_parse.get_text()\n",
    "                    tokens = nltk.word_tokenize(title)\n",
    "                    tokens.extend(nltk.word_tokenize(text))\n",
    "                    toks = []\n",
    "                    for token in tokens:\n",
    "                        toks.append(ps.stem(token))\n",
    "                    # save result\n",
    "                    res = {'url': next_url, 'title': title, 'tokens': toks, 'id': index}\n",
    "                    index += 1\n",
    "                    results.append(res)\n",
    "                    for a in r_parse.find_all('a'):\n",
    "                        if 'href' in a.attrs:\n",
    "                            l = a['href']\n",
    "                            if l.startswith('https') and l not in visited:\n",
    "                                enqueue(frontier, l)\n",
    "            else:\n",
    "                print('could not crawl at ' + next_url)\n",
    "        except:\n",
    "            print(f'woops at {next_url}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build inverted index of crawled pages\n",
    "inv_index = {}\n",
    "for e in results:\n",
    "    d = {}\n",
    "    for t in e['tokens']:\n",
    "        if t in d.keys():\n",
    "            d[t] += 1\n",
    "        else:\n",
    "            d[t] = 1\n",
    "    for k in d:\n",
    "        if k in inv_index.keys():\n",
    "            inv_index[k][e['id']] = d[k]\n",
    "        else:\n",
    "            inv_index[k] = {e['id']: d[k]}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Load inverted index from file\n",
    "import json\n",
    "with open(\"inverted_index.json\", \"r\") as infile:\n",
    "          inv_index = json.load(infile)\n",
    "with open(\"crawled.json\", \"r\") as infile:\n",
    "          results = json.load(infile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bruger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "ps=nltk.PorterStemmer()\n",
    "# AND-merge list of ids\n",
    "def andMerge(l, m):\n",
    "    result = []\n",
    "    if len(l) == 0 or len(m) == 0:\n",
    "        return result\n",
    "    i = 0\n",
    "    j = 0\n",
    "    ie = l[i]\n",
    "    je = m[j]\n",
    "    cont = True\n",
    "    while cont:\n",
    "        k = ie-je\n",
    "        if k == 0:\n",
    "            # Ids match\n",
    "            result.append(ie)\n",
    "            if len(l) - 1 > i and len(m) - 1 > j:\n",
    "                # Take a step in both lists\n",
    "                i += 1\n",
    "                j += 1\n",
    "                ie = l[i]\n",
    "                je = m[j]\n",
    "            else:\n",
    "                cont = False\n",
    "        # Take a step in the list with the smallest id\n",
    "        elif k < 0 and len(l)-1 > i:\n",
    "            i += 1\n",
    "            ie = l[i]\n",
    "        elif k > 0 and len(m)-1 > j:\n",
    "            j += 1\n",
    "            je = m[j]\n",
    "        else:\n",
    "            cont = False\n",
    "    return result\n",
    "\n",
    "def get_ids(dic):\n",
    "    res = list(dic.keys())\n",
    "    res.remove(\"*idf\")\n",
    "    res = [int(i) for i in res]\n",
    "    return res\n",
    "\n",
    "def search(searchstring):\n",
    "    # Split string into words\n",
    "    words = searchstring.split(\" \")\n",
    "    imm = []\n",
    "    # Process words\n",
    "    for w in words:\n",
    "        imm.append(ps.stem(w))\n",
    "    words = imm\n",
    "    ids = []\n",
    "    if len(words) == 1:\n",
    "        # If there is only one word, simply return that list\n",
    "        w = words[0]\n",
    "        if w in inv_index.keys():\n",
    "            ids = get_ids(inv_index[w])\n",
    "    else:\n",
    "        # Merge multiple words\n",
    "        w = words.pop(0)\n",
    "        lis = []\n",
    "        if w in inv_index.keys():\n",
    "            lis = get_ids(inv_index[w])\n",
    "        w = words.pop(0)\n",
    "        lim = []\n",
    "        if w in inv_index.keys():\n",
    "            lim = get_ids(inv_index[w])\n",
    "        imm = andMerge(lis,lim)\n",
    "        for word in words:\n",
    "            if word in inv_index.keys():\n",
    "                lim = get_ids(inv_index[word])\n",
    "            else:\n",
    "                lim = []\n",
    "            imm = andMerge(imm, lim)\n",
    "        ids = imm\n",
    "    res = []\n",
    "    # Translate ids to urls\n",
    "    for i in ids:\n",
    "        for e in results:\n",
    "            if e['id'] == i:\n",
    "                res.append(e['url'])\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "['https://www.loveandlemons.com/easy-dinner-ideas/',\n 'https://www.loveandlemons.com/cabbage-soup/',\n 'https://www.loveandlemons.com/nachos-recipe/']"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"love and lemons beef\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}