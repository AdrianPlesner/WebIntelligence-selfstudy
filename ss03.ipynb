{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Study 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This self study concludes our first \"miniproject\" on crawling and search. The tasks for this self study are:\n",
    "- modify/extend the inverted index you constructed in the previous self study to contain for all postings the term frequencies (if your documents are just the titles of the web pages, you will see very few term frequencies larger than 1, but do not worry about that).\n",
    "- calculate the idf values for all terms, and also include them in your index (cf. slide 3.20 for a schematic view)\n",
    "- implement ranked retrieval as described on slides 3.19 and 3.20 for the ntc.bnc similarity metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bruger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from queue import SimpleQueue as Queue\n",
    "import random\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "ps=nltk.PorterStemmer()\n",
    "\n",
    "\n",
    "# Keep lists of visited pages and a result of urls and titles\n",
    "visited = []\n",
    "results = []\n",
    "\n",
    "# Initialize frontier of 10 front queues that are assigned randomly\n",
    "frontier = []\n",
    "for i in range(4):\n",
    "    frontier.append(Queue())\n",
    "\n",
    "def enqueue(qlist, obj):\n",
    "    q = random.choice(qlist)\n",
    "    q.put(obj)\n",
    "\n",
    "\n",
    "\n",
    "# Keep back queues as dictionary\n",
    "backQ = {}\n",
    "\n",
    "# Extract the next url crawl from back queues\n",
    "def get_crawl(qd):\n",
    "    result = None\n",
    "    keys = list(qd)\n",
    "    i = 0\n",
    "    sec2 = timedelta(0,2)\n",
    "    # Search through each back queue\n",
    "    keys.sort(key=lambda x: qd[x]['time'])\n",
    "    while result is None and i < len(keys):\n",
    "        key = keys[i]\n",
    "        if not qd[key]['queue'].empty() and datetime.now() > qd[key]['time']:\n",
    "            # This queue is not empty and the timestamp permits\n",
    "            result = qd[key]['queue'].get()\n",
    "            # Update with new timestamp\n",
    "            qd[key]['time'] = datetime.now() + sec2\n",
    "\n",
    "        else:\n",
    "            i += 1\n",
    "    return result\n",
    "\n",
    "# Approximate host domain\n",
    "def extract_domain(url):\n",
    "    s = url.split(\"/\")\n",
    "    return s[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define start seed\n",
    "seed = [\"https://pinchofyum.com/\", \"https://loveandlemons.com/\",\"https://imdb.com/\", \"https://gamegrumps.com/\", \"https://www.aau.dk/\"]\n",
    "\n",
    "# Put seeds in frontier\n",
    "for s in seed:\n",
    "    enqueue(frontier, s)\n",
    "\n",
    "index = 0\n",
    "\n",
    "while len(results) < 1000:\n",
    "    next_url = get_crawl(backQ)\n",
    "    if not backQ or get_crawl(backQ) is None:\n",
    "        # If all back queues are empty, refill by emptying front queues\n",
    "        for f in frontier:\n",
    "            while not f.empty():\n",
    "                url = f.get()\n",
    "                domain = extract_domain(url)\n",
    "                if domain not in backQ.keys():\n",
    "                    # Add new back queue if one for this domain does not exist\n",
    "                    backQ[domain] = {'time': datetime.now(), 'queue': Queue()}\n",
    "                backQ[domain]['queue'].put(url)\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            # Logic for crawling a page\n",
    "            if next_url in visited or next_url is None:\n",
    "                continue\n",
    "            print('crawling at ' + next_url)\n",
    "            # Initialize robotfile parser\n",
    "            rp=RobotFileParser()\n",
    "            rp.set_url(next_url)\n",
    "            rp.read()\n",
    "            # Check if robots.txt allows\n",
    "            if rp.can_fetch(\"*\", next_url):\n",
    "                r=requests.get(next_url)\n",
    "                visited.append(next_url)\n",
    "                #extract title\n",
    "                r_parse = BeautifulSoup(r.text, 'html.parser')\n",
    "                title = r_parse.find('title')\n",
    "                if title is not None:\n",
    "                    title = title.string\n",
    "                    # Get text from page and tokenize\n",
    "                    text = r_parse.get_text()\n",
    "                    tokens = nltk.word_tokenize(title)\n",
    "                    tokens.extend(nltk.word_tokenize(text))\n",
    "                    toks = []\n",
    "                    for token in tokens:\n",
    "                        toks.append(ps.stem(token))\n",
    "                    # save result\n",
    "                    res = {'url': next_url, 'title': title, 'tokens': toks, 'id': index}\n",
    "                    index += 1\n",
    "                    results.append(res)\n",
    "                    for a in r_parse.find_all('a'):\n",
    "                        if 'href' in a.attrs:\n",
    "                            l = a['href']\n",
    "                            if l.startswith('https') and l not in visited:\n",
    "                                enqueue(frontier, l)\n",
    "            else:\n",
    "                print('could not crawl at ' + next_url)\n",
    "        except:\n",
    "            print(f'woops at {next_url}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate idf value for a term and index\n",
    "def idf(term, idx):\n",
    "    if term in idx.keys():\n",
    "        return math.log10( len(idx) / len(idx[term]))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Build index\n",
    "inv_index = {}\n",
    "for e in results:\n",
    "    d = {}\n",
    "    for t in e['tokens']:\n",
    "        if t in d.keys():\n",
    "            d[t] += 1\n",
    "        else:\n",
    "            d[t] = 1\n",
    "    for k in d:\n",
    "        if k in inv_index.keys():\n",
    "            inv_index[k][e['id']] = d[k]\n",
    "        else:\n",
    "            inv_index[k] = {e['id']: d[k]}\n",
    "\n",
    "# Insert idf values in index\n",
    "for term in inv_index.keys():\n",
    "    inv_index[term]['*idf'] = idf(term, inv_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"crawled.json\", \"r\") as infile:\n",
    "          crawled = json.load(infile)\n",
    "\n",
    "with open(\"inverted_index.json\", \"r\") as infile:\n",
    "          inv_index = json.load(infile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Ranked search on text match\n",
    "def search(query):\n",
    "    # Preprocess query\n",
    "    words = query.split(\" \")\n",
    "    stemmed = []\n",
    "    for w in words:\n",
    "        stemmed.append(ps.stem(w))\n",
    "    words = np.array(stemmed)\n",
    "\n",
    "    docs = {}\n",
    "    i = 0\n",
    "    # Build vector for each document with matching words\n",
    "    for w in words:\n",
    "        if w in inv_index.keys():\n",
    "            d = inv_index[w]\n",
    "            for k in d.keys():\n",
    "                if k != '*idf':\n",
    "                    if k not in docs.keys():\n",
    "                        docs[k] = np.zeros(len(words))\n",
    "                    docs[k][i] = d[k] * d['*idf']\n",
    "        i += 1\n",
    "    # Transform to unit vectors\n",
    "    for k in docs.keys():\n",
    "        v = docs[k]\n",
    "        vlen = np.sqrt(np.sum(np.apply_along_axis(lambda x: x**2,0, v)))\n",
    "        docs[k] = v / vlen\n",
    "    results = []\n",
    "    # Construct query vector\n",
    "    words = np.ones(len(words))\n",
    "    wlen = np.sqrt(np.sum(np.apply_along_axis(lambda x: x**2,0, words)))\n",
    "    words = words /wlen\n",
    "    # Calculate cosine similarity for each document vetor\n",
    "    for k in docs.keys():\n",
    "        results.append((k, np.dot(docs[k], words)))\n",
    "    # Sort and select top 10\n",
    "    results.sort(reverse=True, key=lambda x: x[1])\n",
    "    results = results[:10]\n",
    "    print(results)\n",
    "    rep = []\n",
    "    # Translate ids to urls\n",
    "    for i in range(len(results)):\n",
    "        t = int(results[i][0])\n",
    "        rep.append(crawled[t]['url'])\n",
    "    return rep"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('870', 0.9956356178015583), ('0', 0.9023519251550977), ('43', 0.8373621693312493), ('37', 0.8164050363872593), ('56', 0.8164050363872593), ('136', 0.8164050363872593), ('232', 0.8164050363872593), ('239', 0.8164050363872593), ('706', 0.8164050363872593), ('971', 0.8164050363872593)]\n"
     ]
    },
    {
     "data": {
      "text/plain": "['https://www.loveandlemons.com/chickpea-salad-sandwich/',\n 'https://loveandlemons.com/',\n 'https://www.loveandlemons.com/easy-dinner-ideas/',\n 'https://www.loveandlemons.com/butternut-squash-soup/',\n 'https://www.loveandlemons.com/how-to-cook-brown-rice/',\n 'https://www.cedarsfoods.com/',\n 'https://www.loveandlemons.com/healthy-lunch-ideas/',\n 'https://www.loveandlemons.com/farro/',\n 'https://www.loveandlemons.com/focaccia/',\n 'https://www.loveandlemons.com/stuffed-zucchini-boats/']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"pasta tomato meat\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}